{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7o1Je1x0XQU"
   },
   "source": [
    "# Mount drive and append path to PYTONPATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9vhq3fcEHwj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "sys.path.append(\"/content/drive/MyDrive/DeepLCMS/train_google_colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B10ISUtcE4nE"
   },
   "source": [
    "# Import and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaleshIpENkS"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install lightning\n",
    "!pip install timm\n",
    "!pip install torchinfo\n",
    "!pip install scikit-posthocs\n",
    "!pip install optuna\n",
    "!pip install torchcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQbyJQmAXznU"
   },
   "outputs": [],
   "source": [
    "import colab_functions\n",
    "import colab_utils\n",
    "import pandas as pd\n",
    "import prepare_data\n",
    "import timm\n",
    "import train_NN\n",
    "from lightning.pytorch import loggers, callbacks, tuner, trainer\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvVdxwYfiHwl"
   },
   "outputs": [],
   "source": [
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjCM3Grt0NWP"
   },
   "source": [
    "# Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYo52Rm30Q-k"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "!unzip -q \"*.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqmzdwGyVGvJ"
   },
   "source": [
    "# Check if GPU is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jh0cGdllMv8h"
   },
   "outputs": [],
   "source": [
    "device = colab_functions.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWU7V2tIBhAl"
   },
   "source": [
    "# Getting a tunable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDdrLD2CBaWu"
   },
   "outputs": [],
   "source": [
    "class Resnet_model_tune(pl.LightningModule):\n",
    "    def __init__(self, hyperparameters):\n",
    "        super().__init__()\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.model = create_model(\"resnet50d.a3_in1k\", pretrained=True, num_classes=1)\n",
    "\n",
    "        # Freeze all layers except for the last one\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.hyperparameters[\"dropout\"]),\n",
    "            nn.Linear(in_features=512, out_features=256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=1, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        y_pred_logits = self(x).squeeze()\n",
    "        y_pred = torch.sigmoid(y_pred_logits)\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        y_pred_class = torch.round(y_pred)\n",
    "        acc = (y_pred_class == y).sum().item() / len(y_pred)\n",
    "        self.log(\n",
    "            \"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=False, logger=True\n",
    "        )\n",
    "        # Calculate F1\n",
    "        metric_f1 = BinaryF1Score().to(y.device)\n",
    "        f1 = metric_f1(y_pred_class, y)\n",
    "        self.log(\n",
    "            \"train_f1\", f1, on_step=False, on_epoch=True, prog_bar=False, logger=True\n",
    "        )\n",
    "        # Calculate Precision\n",
    "        metric_precision = BinaryPrecision().to(y.device)\n",
    "        precision = metric_precision(y_pred_class, y)\n",
    "        self.log(\n",
    "            \"train_precision\",\n",
    "            precision,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        # Calculate Recall\n",
    "        metric_f1 = BinaryRecall().to(y.device)\n",
    "        recall = metric_f1(y_pred_class, y)\n",
    "        self.log(\n",
    "            \"train_recall\",\n",
    "            recall,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        y_pred_logits = self(x).squeeze()\n",
    "        y_pred = torch.sigmoid(y_pred_logits)\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        y_pred_class = torch.round(y_pred)\n",
    "        acc = (y_pred_class == y).sum().item() / len(y_pred)\n",
    "        self.log(\n",
    "            \"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        # Calculate F1\n",
    "        metric_f1 = BinaryF1Score().to(y.device)\n",
    "        f1 = metric_f1(y_pred_class, y)\n",
    "        self.log(\"val_f1\", f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # Calculate Precision\n",
    "        metric_precision = BinaryPrecision().to(y.device)\n",
    "        precision = metric_precision(y_pred_class, y)\n",
    "        self.log(\n",
    "            \"val_precision\",\n",
    "            precision,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        # Calculate Recall\n",
    "        metric_f1 = BinaryRecall().to(y.device)\n",
    "        recall = metric_f1(y_pred_class, y)\n",
    "        self.log(\n",
    "            \"val_recall\",\n",
    "            recall,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, list):\n",
    "            # Assuming the first element in the list is the input tensor\n",
    "            input_tensor = batch[0]\n",
    "            return self(input_tensor)\n",
    "        else:\n",
    "            # If batch is already a tensor, proceed as usual\n",
    "            print(\"Input Shape:\", batch.shape)\n",
    "            return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "\n",
    "        if self.hyperparameters[\"optimizer\"] == \"Adam\":\n",
    "            optimizer = Adam(\n",
    "                self.parameters(), lr=self.hyperparameters[\"lr\"], weight_decay=2e-5\n",
    "            )\n",
    "        elif self.hyperparameters[\"optimizer\"] == \"SGD\":\n",
    "            optimizer = SGD(\n",
    "                self.parameters(), lr=self.hyperparameters[\"lr\"], weight_decay=2e-5\n",
    "            )\n",
    "        elif self.hyperparameters[\"optimizer\"] == \"RMSprop\":\n",
    "            optimizer = RMSprop(\n",
    "                self.parameters(), lr=self.hyperparameters[\"lr\"], weight_decay=2e-5\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported optimizer: {self.hyperparameters['optimizer']}\"\n",
    "            )\n",
    "\n",
    "        scheduler = None\n",
    "\n",
    "        if self.hyperparameters[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer, mode=\"min\", factor=0.1, patience=3\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "            }\n",
    "        elif self.hyperparameters[\"scheduler\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hyperparameters = {\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"]),\n",
    "        \"scheduler\": trial.suggest_categorical(\n",
    "            \"scheduler\", [\"ReduceLROnPlateau\", \"CosineAnnealingLR\"]\n",
    "        ),\n",
    "        \"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-1),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.01, 1),\n",
    "    }\n",
    "\n",
    "    model = Resnet_model_tune(hyperparameters)\n",
    "    logger = CSVLogger(\"logs\", name=str(trial.number))\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        max_epochs=50,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=10),\n",
    "            PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "\n",
    "def print_callback(study, trial):\n",
    "    print(\n",
    "        f\"Trial {trial.number} finished with value: {trial.value} and parameters: {trial.params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yI_ZHuioBaZZ"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    preprocess_train,\n",
    "    preprocess_val,\n",
    "    preprocess_test,\n",
    ") = prepare_data.get_timm_transforms(train_NN.Resnet_model())\n",
    "\n",
    "(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    ") = prepare_data.get_dataloaders(\n",
    "    preprocess_train=preprocess_train,\n",
    "    preprocess_val=preprocess_val,\n",
    "    preprocess_test=preprocess_test,\n",
    ")\n",
    "\n",
    "# bug encountered:\n",
    "# https://github.com/pytorch/pytorch/issues/67978\n",
    "study.optimize(objective, n_trials=100, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sE2UYgisObM"
   },
   "outputs": [],
   "source": [
    "with open(\"optuna_params.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(study.best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"optuna_params.pickle\", \"rb\") as handle:\n",
    "    optuna_params = pickle.load(handle)\n",
    "\n",
    "print(study.best_params == optuna_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMH7y9ZLRGwv"
   },
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIh2lYtcRGzS"
   },
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD9Lb12mRG1b"
   },
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBRV598rLweo"
   },
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAvN08b5uQZB"
   },
   "outputs": [],
   "source": [
    "results_df = colab_functions.get_experiment_results().assign(\n",
    "    experiment=lambda df: df.experiment.astype(int)\n",
    ")\n",
    "optuna_trials = pd.read_csv(\"optuna_trials.csv\")\n",
    "\n",
    "# merge results_df with optuna_trials so that we have access to the full training\n",
    "# data with all epochs\n",
    "# this is needed since optuna made a decision based on overfitted data\n",
    "\n",
    "df = results_df.merge(optuna_trials, left_on=\"experiment\", right_on=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubkUof9BNK2f"
   },
   "source": [
    "Optuna originally selected trial #27, which only achieved a validation loss of 0.1353. This is because Optuna considers the validation loss of the last epoch before terminating the trial due to overfitting. Therefore, the final conclusion reached by Optuna is based on an already overfitted model. Based on the learning curves logged, we can determine the best conditions and the number of epochs we should train our model for.\n",
    "\n",
    "These are the validation losses regarding trial #27:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUR3B4VWNPyo"
   },
   "outputs": [],
   "source": [
    "(df.query(\"experiment == 27 and variable.str.contains('val_loss')\")).sort_values(\n",
    "    by=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYF0dXZdORMV"
   },
   "source": [
    "The absolute minimum validation loss reached during the optimization procedure was 0.1289 at epoch 3 for experiment 12. However, by the next epoch, the model was overfitted and recorded a final validation loss of 0.2212, which is higher than the validation loss of trial #27. Therefore, trial #27 was selected over experiment 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0P9ZOD4Cl9C"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.loc[lambda df: df.groupby(\"variable\")[\"value_x\"].idxmin(), :].query(\n",
    "        \"variable.str.contains('val_loss') \"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-QW2qJKO6LO"
   },
   "source": [
    "These experiments represent the overall best trials and corresponding epochs per metric monitored. As you can see, trial #12 is still present with a validation accuracy of 0.98. Most of these trials used the `Adam` optimizer and `CosineAnnealingLR` scheduler. Based on these findings, we can opt for the conditions described for trial #12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPKK3I0yCzs0"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.loc[lambda df: df.groupby(\"variable\")[\"value_x\"].idxmax(), :].query(\n",
    "        \"~variable.str.contains('train|loss')\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1NvPurIvAG-FhiUDynbwQfC86juRedJ73",
     "timestamp": 1701721337593
    },
    {
     "file_id": "1oe2LssF1fb7Z3pRCP_eEHJ31sKeevypj",
     "timestamp": 1701632327520
    },
    {
     "file_id": "1FxpkTpMoivSD7nQRkqJsjJ_UhxNTpzed",
     "timestamp": 1700942428337
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
