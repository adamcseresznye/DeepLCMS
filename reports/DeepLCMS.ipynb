{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb309f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'DeepLCMS: Leveraging transfer learning for the classification of pseudo images in mass spectrometry-based analysis'\n",
    "author: Adam Cseresznye\n",
    "date: '2023-12-23'\n",
    "bibliography: DeepLCMS.bib\n",
    "categories:\n",
    "  - DeepLCMS\n",
    "jupyter: python3\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a692-e95d-4785-8d85-ed1cc10b4056",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Representative examples of DeepLCMS predictions accompanied by their corresponding probability estimates.](exp-5-prediction_matrix.png){fig-align=\"center\" width=50%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955da8b-ac66-47c9-95ba-2c66f68fe821",
   "metadata": {},
   "source": [
    "Welcome to DeepLCMS, a project that combines mass spectrometry analysis with the power of deep learning models!\n",
    "\n",
    "Unlike traditional methods, DeepLCMS eliminates the need for extensive data processing, including peak alignment, data annotation, quantitation, and other time-consuming steps. Instead, it relies on the power of deep learning to directly classify mass spectrometry-based pseudo-images with high accuracy. To demonstrate the capabilities of pre-trained neural networks for high-resolution LC/MS data, we successfully apply our convolutional neural network (CNN) to categorize substance abuse cases. We utilize the openly available Golestan Cohort Study's metabolomics HRMS/LC data to train and evaluate our CNN [@pourshams_cohort_2010; @ghanbari_metabolomics_2021; @li_untargeted_2020]. We also delve into the network's decision-making process through TorchCam library. This tool allows us to gain insights into the factors that influence the network's classifications, helping us identify key compound classes that play a crucial role in differentiating between classes. By analyzing retention time and molecular weight, we can pinpoint areas of interest within the data.\n",
    "\n",
    "DeepLCMS paves the way for a new era of mass spectrometry analysis, offering a faster, more efficient, and more insightful approach to data interpretation. Its ability to directly classify pseudo-images without extensive preprocessing opens up a world of possibilities for researchers and clinicians alike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4698f-d15a-4484-a3ff-692eb1e0685b",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "**At a glance**\n",
    "\n",
    "The DeepLCMS project aims to provide researchers with a reproducible source code for leveraging deep learning for mass spectrometry data analysis. It distinguishes itself from previous studies by:\n",
    "*Comparing Diverse* Architecture Families: Assessing a broader range of architecture families to find the most suitable one, including cutting-edge architectures like vision transformers.\n",
    "*Hyperparameter Tuning*: Conducting basic hyperparameter tuning to optimize the learning rate using Optuna including optimizer, and learning rate scheduler – crucial aspects beyond the architecture itself.\n",
    "*Image Quality Analysis*: Investigating the impact of image quality on validation metrics, examining image sharpness and data augmentation imitating retention time shift.\n",
    "*Regularization Techniques*: Employing regularization techniques like random-tilting images and random erasing during training to improve model generalization.\n",
    "*Interpreting Pretrained Network Decisions*: Analyzing how the pre-trained network makes its decisions using TorchVision.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621f716-2c9d-4464-9f74-9a2d78a163c3",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d39a8-0d13-4104-aa4d-42ce70e259d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af189aa",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821aab42-6ea3-4277-9c47-01586fc1b53f",
   "metadata": {},
   "source": [
    "While computer vision has gained widespread adoption in various aspects of our lives[@dobson_birth_2023], its application in medical imaging and biosciences has lagged behind, primarily due to limitations in clinical dataset size, accessibility, privacy concerns, experimental complexity, and high acquisition costs. For such applications, transfer learning has emerged as a potential solution[@seddiki_towards_2020]. This technique is particularly effective with small datasets, requiring fewer computational resources while achieving good classification accuracy compared to models trained from scratch. Transfer learning involves a two-step process. Initially, a robust data representation is learned by training a model on a dataset comprising a vast amount of annotated data encompassing numerous categories (ImageNet for example). This representation is then utilized to construct a new model based on a smaller annotated dataset containing fewer categories. \n",
    "\n",
    "## Application of Pretrained Neural Networks for Mass Spectrometry Data\n",
    "\n",
    "The use of pre-trained neural networks for mass spectrometry data analysis is relatively new, with only a handful of publications available to date. These studies have demonstrated the potential of deep learning models to extract meaningful information from raw mass spectrometry data and perform predictive tasks without the need for extensive data processing as required by the traditional workflows.\n",
    "\n",
    "## Previous Research\n",
    "\n",
    "* In 2018, @behrmann_deep_2018 used deep learning techniques for tumor classification in Imaging Mass Spectrometry (IMS) data.\n",
    "\n",
    "* In 2020, @seddiki_towards_2020 utilized MALDI-TOF images of rat brain samples to assess the ability of three different CNN architectures – LeNet, Lecun, and VGG9 – to differentiate between different types of cancers based on their molecular profiles.\n",
    "\n",
    "* In 2021, @cadow_feasibility_2021 explored the use of pre-trained networks for the classification of tumors from normal prostate biopsies derived from SWATH-MS data. They delved into the potential of deep learning models for analyzing raw mass spectrometry data and performing predictive tasks without the need for protein quantification. To process raw MS images, the authors employed pre-trained neural network models to convert them into numerical vectors, enabling further processing. They then compared several classifiers, including logistic regression, support vector machines, and random forests, to accurately predict the phenotype.\n",
    "\n",
    "* In 2022, @shen_deep_2022 released deepPseudoMSI, a deep learning-based pseudo-mass spectrometry imaging platform, designed to predict the gestational age in pregnant women based on LC-MS-based metabolomics data. This application consists of two components: Pseudo-MS Image Converter: for converting LC-MS data into pseudo-images and the deep learning model itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e76a6e-4270-4af3-adbf-2294b5d81ce1",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Project Structure\n",
    "\n",
    "To accommodate the high computational demands of neural network training, the DeepLCMS project is divided into two main parts. The first part focuses on data preprocessing, specifically converting LC/MS data into pseudo-images using the PyOpenMS library, which is written in C++ and optimized for efficiency. This task can be handled on a CPU, and the corresponding source code is found in the `src/deeplcms_functions` directory.\n",
    "\n",
    "To effectively train the neural networks that demand GPU acceleration, the project employs the PyTorch Lightning framework, a comprehensive solution for building and deploying deep learning models. Training experiments are conducted within Jupyter Notebooks hosted on Google Colab, a cloud platform equipped with free GPU access. The training code and corresponding modules reside within the `src/train_google_colab` directory. This folder seamlessly integrates with Google Colab, allowing for effortless module imports.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd2970-43b4-4b7b-8c49-2228b0ade984",
   "metadata": {},
   "source": [
    "# Materials and Methods\n",
    "## Dataset\n",
    "\n",
    "To ensure the feasibility of our proof-of-concept demonstration, we selected a suitable dataset from the Metabolomics Workbench. We prioritized studies with distinct groups and a minimum sample size of 200. Additionally, we chose a dataset with a disk requirement of less than 50 GB to minimize computational resource demands. Based on these criteria, we identified the Golestan Cohort Study [@pourshams_cohort_2010]. This study, conducted in northeastern Iran, primarily investigates the risk factors for upper gastrointestinal cancers in this high-risk region. Approximately 50,000 volunteers were analyzed, including opium users and their mortality outcomes. Quantitative targeted liquid chromatography mass spectrometric (LC-MS/MS) data was collected at the University of North Carolina at Chapel Hill [@ghanbari_metabolomics_2021; @li_untargeted_2020]. The dataset consisted of 218 opioid users and 80 non-users. After initial data inspection and conversion to mzML format using the ProteoWizard 3.0.22155 software, files were divided into training (n = 214), validation (n = 54), and test (n = 30) sets. To evaluate the impact of image characteristics and augmentation techniques on classification performance, four datasets were prepared. One dataset employed a bin size of 500 × 500 using the numpy library's histogram2d function and was named `ST001618_Opium_study_LC_MS_500`. Another dataset incorporated data augmentation using the augment_images function, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. This dataset was named `ST001618_Opium_study_LC_MS_500_augmented`. The third dataset employed a higher bin size of 1000 × 1000 using histogram2d for sharper pseudoimages and was named `ST001618_Opium_study_LC_MS_1000`. The final dataset applied the same augmentation technique to this dataset, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. It was named `ST001618_Opium_study_LC_MS_1000_augmented`.\n",
    "\n",
    "## Software\n",
    "\n",
    "The source code for the DeepLCMS project utilizes the following software packages and versions: PyTorch Lightning 2.1.3, Pytorch Image Models (timm)  0.9.12, torchinfo 1.8.0, Optuna 3.5.0, TorchCam 0.4.0, pandas 2.1.3, NumPy 1.26, and Matplotlib 3.7.1.\n",
    "\n",
    "## Creating the pseudo images\n",
    "\n",
    "Mass spectrometry-based pseudo images are generated using the `plot_2D_spectra_overview` function, which creates a 2D heatmap of mass spectrometry data. The function first loads the mass spectrometry data from the provided file path using the OpenMS library. A 2D histogram is created from the data, with the x-axis representing the retention time (RT) and the y-axis representing the m/z values. The intensity values of the histogram are normalized using the highest peak. To enhance the visualization of the data, a Gaussian filter is applied to reduce noise, and the filtered image is scaled to the range 0-255. The histogram is then logarithmically transformed to better visualize large ranges of data. The function then creates a plot of the histogram using matplotlib, with the colormap set to 'jet'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ade18-4cc3-4eca-aa38-48f089a609e1",
   "metadata": {},
   "source": [
    "# Results and Discussion\n",
    "## Selecting a model architecture family\n",
    "\n",
    "From the readily available model architecture families provided by [Pytorch Image Models](https://github.com/huggingface/pytorch-image-models#models), 68 unique ones were chosen as representative examples of a given class and filtered based on their parameter count, selecting those with parameters counts between 10 and 20 million to allow for an unbiased comparison. Out of the 68 ones selected 32 were subsequently underwent training with validation metrics recorded. According to @tbl-exp-1-result (arranged according to validation loss), the MobileOne S3 emerged as the top performer with an F1 score of 0.95. It was followed by DenseNet (F1 = 0.92), MobileViTV2 (F1 = 0.90), and ConvNeXt Nano (F1 = 0.84). RepVit M3 rounded out the top five with an F1 score of 0.86. To delve deeper into the performance of each architecture family, we evaluated all individual architectures within each family (@tbl-exp-1-best_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d41a1-373d-4436-b017-55b87fbab55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: tbl-exp-1-result\n",
    "# | tbl-cap: Validation metrics of model architectural families from the PyTorch Image Models library using models with parameter counts ranging from 10 to 20 million.\n",
    "\n",
    "(\n",
    "    pd.read_csv(\"exp-1-result.csv\")\n",
    "    .rename(\n",
    "        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n",
    "    )\n",
    "    .rename(columns={\"Minimal Param Model Count\": \"Parameter Count (M)\"})\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55c827-5f4b-4a52-9d8b-7df9f3bf3c55",
   "metadata": {},
   "source": [
    "As shown in @tbl-exp-1-best_models, the top four positions based on the validation metrics were occupied by models belonging to the ConvNeXt family. These models, particularly the first two (convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 with validation loss 0.19, convnext_large_mlp.clip_laion2b_augreg_ft_in1k with validation loss 0.22), were pre-trained on the extensive LAION-2B dataset and fine-tuned on ImageNet-1k, enabling them to learn complex patterns that generalize well to unseen data. This comprehensive dataset and model structure contribute to their superior performance in this task. The dominance of the ConvNeXt family is noteworthy, suggesting their effectiveness in handling complex data such as mass spectrometry pseudoimages. Apple's MobileOne also demonstrated remarkable results (validation loss = 0.27), ranking fourth in terms of validation loss. Finally, MobileViT (validation loss = 0.27), a lightweight network, secured the seventh position.  \n",
    "Nevertheless, it is essential to acknowledge that while the ConvNeXt model achieved the highest validation metrics, it is considerably larger and more parameter-intensive. This implies that it demands more computational resources for training and necessitates greater care to prevent potential overfitting, as opposed to other models that are 10-20 times smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e043125-7367-4fd7-817b-d0ef6ce2aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: tbl-exp-1-best_models\n",
    "# | tbl-cap: Top 10 models based on evaluation metrics of all model architectures in families from the PyTorch Image Models library, regardless of their parameter counts.\n",
    "\n",
    "(\n",
    "    pd.read_csv(\"exp-1-best_models.csv\")\n",
    "    .rename(\n",
    "        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n",
    "    )\n",
    "    .round(2)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cf409-b3ef-4e6e-926e-c7060066df19",
   "metadata": {},
   "source": [
    "To further assess the suitability of these models and the consistency of their performance, we trained the top three performing models from each family five consecutive times and calculated the median and standard deviation of their validation metrics (@fig-exp-2-replicates_result). This approach allowed us to identify the models that exhibited the most consistent performance across multiple training runs. The results revealed that the MobileViT2 family consistently performed similarly to the ConvNeXt across all five training runs, except for the validation losses where ConvNeXt exhibited the lowest values with 0.21. According to the replication study, the ConvNeXt emerged as the most effective model, surpassing both MobileOne and MobileViT2. Despite exhibiting similar patterns, ConvNeXt exhibited a statistically significant advantage over MobileViT2 in validation loss (p = 0.01) and achieved significantly better validation recall (p < 0.05). Among the three models tested, MobileOne consistently underperformed its counterparts in all performance metrics, except for validation recall, where it narrowly outperformed MobileViT2 (p < 0.05). In light of these findings, we opted to use the ConvNeXt large model for our research.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86307ea-d9d1-4919-a80f-e603073117de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-2-replicates_result\n",
    "# | fig-cap: \"Median and standard deviation of best validation metrics achieved during consecutive trainings\"\n",
    "\n",
    "Image.open(\"exp-2-replicates_result.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a6a33-d5b7-493f-81ca-8fa0a0a51c9d",
   "metadata": {},
   "source": [
    "## Selecting image properties as hyperparameters\n",
    "\n",
    "Next, we explored the impact of image sharpness and offsets along the x and y axis as a method for image augmentation to replicate the effects of Rt shifts and incidental mass calibration drifts. @fig-exp-3-image_example illustrates how the number of bins employed in the `np.histogram2d` function within the `plot_2D_spectra_overview` function—which serves to transform LC/MS files into pseudo images—influences the image sharpness. Here, the bins parameter signifies the number of bins in both the x and y dimensions, which are represented by nx and ny, respectively. This results in the formation of an nx by ny bin grid across the data for the histogram. By increasing the number of bins (from 500, 500 to 1000, 1000), one could obtain a sharper image, but it could also lead to increased noise and computational complexity. \n",
    "\n",
    "Furthermore, we sought to evaluate the impact of image augmentation, specifically the generation of multiple, modified images prior to image training, on the validation metrics. The `augment_images` function applies random offsets to the specified image, producing a designated number— 9 in our case—of augmented images with a maximum horizontal offset of five and a maximum vertical offset of five pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf80aff-0a61-4df9-8553-3dda70a6b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-3-image_example\n",
    "# | fig-cap: \"Examples of images obtained after applying different bin sizes and offsets as imitation of Rt shift. The amount of offset along the x and y axes is given in the image name. Image quality as a function of the number of bins is set to either 500, indicating a lower-quality image, or 1000, indicating a higher-quality image.\"\n",
    "\n",
    "img_paths = list(Path.cwd().glob(\"exp_3*\"))\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 2, 2\n",
    "\n",
    "for idx, img_path in zip(range(1, rows * cols + 1), img_paths):\n",
    "    fig.add_subplot(rows, cols, idx)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    plt.imshow(img)\n",
    "    plt.title(img_path.stem[6:])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a4c22-7d23-48d5-95da-383d8b6385c6",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "@fig-exp_3_experiment_result_1e 2 illustrates, image sharpness in combination with image augmentation had a significant impact on the validation loss. Setting the bin size to 500 and subsequent augmentation resulted in the lowest validation loss (0.129) and the highest validation F1 (0.982). Based on these findings, less image sharpness and more training images, obtained through data augmentation prior to training, had a beneficial effect on the validation metrics. Taken together, the final training and fine-tuning should be conducted on images with a bin size set to 500 and subsequent augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316f785-fb96-40b8-a963-48d892215f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp_3_experiment_result_1\n",
    "# | fig-cap: \"Training and validation metrics of experiments involving the Selection of image properties as hyperparameters\"\n",
    "\n",
    "Image.open(\"exp_3_experiment_result_1.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececfd5a-96bb-4f29-b9d4-a74e9fbd2f9b",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with Optuna\n",
    "\n",
    "Optuna, an open-source hyperparameter optimization framework, delivers an automated and efficient approach for optimizing hyperparameters in machine learning and deep learning models. The framework operates by defining an objective function that needs to be maximized or minimized. This function encapsulates the model and the metric to be optimized. Within the objective function, hyperparameters are suggested using a trial object. Optuna supports various types of hyperparameters, including float, integer, and categorical. A study object is created and the optimize method is invoked, specifying the number of trials. The core of Optuna's operation lies in its optimization process, which employs state-of-the-art algorithms to search the hyperparameter space efficiently. It also prunes unpromising trials to conserve computational resources. This is where Bayesian optimization plays a crucial role, guiding the search based on information from previous trials. The best hyperparameters found during the search can be obtained from the study object. Optuna also provides a real-time web dashboard for tracking the optimization process, allowing users to monitor the progress of the optimization and identify potential bottlenecks.  \n",
    "To optimize the model's performance, we employed Optuna to identify the most effective optimizer from a pool of four: Adam, AdamW, Adamax, and RMSprop. Additionally, we sought the optimal learning rate scheduler, choosing between CosineAnnealingLR, based on the validation loss achieved during training. After extensive optimization, the combination of Adamax optimizer and CosineAnnealingLR learning rate scheduler yielded the lowest validation loss (0.23), solidifying its selection for subsequent experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609d9bc-ef04-4c93-81f5-a7de8dc89ab9",
   "metadata": {},
   "source": [
    "## Evaluating the final model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49689a7c-8627-489d-bc16-d1c568404de2",
   "metadata": {},
   "source": [
    "The final model training was conducted using the optimal settings determined during the optimization experiments, employing transfer learning of a pre-trained ConvNeXt large model, with all of its layers frozen except for the last classification head. The learning rate was automatically determined to be 0.006 based on the learning rate finder tool developed by PyTorch Lightning. For the training, we utilized the augmented dataset (fig-exp-5-transformed_grid) that featured bin size set to 500 for both dimensions. To prevent overfitting and promote better generalization capabilities, random erasing was employed with a probability of 1, and color jitter was also utilized (probability=0.25). This technique effectively augments the training data, prompting the model to prioritize more essential features and minimize its dependence on specific regions of the image. This prevents the model from memorizing limited patterns in the training data and encourages it to generalize better to unseen data. The underlying concept resembles a dropout layer, a common component of neural network architectures. The optimized model achieved validation metrics with loss of 0.138, validation precision of 0.963, F1 0.946, and accuracy exceeding 0.944, and validation recall reaching 0.93.\n",
    "\n",
    "During training, we employed early stopping to prevent overfitting, terminating training if the validation loss fails to improve for 10 consecutive epochs. Additionally, we utilized PyTorch Lightning's built-in ModelCheckpoint to save the best-performing model, ensuring that the model that achieved the highest validation metrics was preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29d5b9-c510-4a92-8f56-22b91b05db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-5-transformed_grid\n",
    "# | fig-cap: \"Example of augmented dataloader images used during training time, demonstrating the application of random erasing and color jitter to reduce overfitting\"\n",
    "\n",
    "Image.open(\"exp-5-transformed_grid.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2ea33-fd68-4fd2-92a2-cd3458b236e0",
   "metadata": {},
   "source": [
    "After the training was completed, the best-performing model was reloaded, and test metrics were calculated using the 30 test samples present in our dataset. According to the final evaluation, the model produced 20 True Positives, 7 True Negatives, 2 False Negatives, and 1 False Positive. These results translate to a precision of 0.952, recall 0.909, F1 0.930, and accuracy 0.90. While the model achieved impressive validation metrics, the slight drop in performance on the test set indicates a potential overfitting issue. Further optimization of the model, particularly during the method optimization phase, could be achieved through cross-validation strategies. However, this approach might necessitate excessive computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7dbc50-3dd7-4cc9-9ee8-536a09ae305f",
   "metadata": {},
   "source": [
    "## Model interpretability \n",
    "\n",
    "To assess the model's confidence in its predictions, we can transform the raw output, which is the binary cross-entropy loss derived from a set of logits obtained using PyTorch's BCEWithLogitsLoss function, into prediction probabilities using a sigmoid function. These converted values reflect the probability that the model assigns to a given prediction belonging to Class 1 (Opioid User). @fig-exp-5-prediction_matrix illustrates this concept, showing that in the first instance, the model estimates a 43% probability of the instance belonging to Class 1, suggesting a higher likelihood of it being a Non-Opioid User (and this prediction proved correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31653b8-e2c2-4976-8ff7-a9b49f754003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-5-prediction_matrix\n",
    "# | fig-cap: \"Set of images from the test set, along with their corresponding prediction probabilities for the Class 1 (Opidid User). The true labels are displayed beside the predictions. Green labels indicate successful predictions, whereas red labels denote unsuccessful ones\"\n",
    "\n",
    "Image.open(\"exp-5-prediction_matrix.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd193e-dfc5-4002-8b93-36c8e005bca3",
   "metadata": {},
   "source": [
    "To gain insights into the regions of an image our model focuses on when making a prediction, we employed LayerCAM, a tool from the TorchCam package. LayerCAM (@jiang_layercam_2021), short for Layer Class Activation Maps, is a technique for generating class activation maps from distinct layers of a CNN. These regions of interests highlight the key features that the model utilizes to differentiate between opioid users and non-opioid users. Class activation maps are typically derived from the final convolutional layer of a CNN, highlighting discriminative object regions for the class of interest. Such a tool would be particularly valuable for validating the insights derived from our deep learning model, particularly when compared to more targeted or untargeted metabolomics data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b1b1e-d8d2-48ce-99c7-1445eaae130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-5-plot_activation\n",
    "# | fig-cap: \"Regions of interest identified by LayerCAM, demonstrating the specific areas of the image that our model deems most relevant for making predictions.\"\n",
    "\n",
    "Image.open(\"exp-5-plot_activation.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47a643-3579-45b2-ad40-a24264c66ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def overlay_untargeted_data(\n",
    "    query_str, ms_dial_location, background_image_location, crop_area\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlay untargeted metabolomics data on a background image.\n",
    "\n",
    "    Parameters:\n",
    "    - query_str (str): Query string for filtering the data.\n",
    "    - ms_dial_location (str): Location of the mass spectrometry data file.\n",
    "    - background_image_location (str): Location of the background image.\n",
    "    - crop_area (tuple): Coordinates for cropping the background image (left, upper, right, lower).\n",
    "\n",
    "    Returns:\n",
    "    - composite_img (PIL.Image.Image): Overlay of data on the background image.\n",
    "    - df (pd.DataFrame): Filtered and processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Read and preprocess the mass spectrometry data\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            ms_dial_location,\n",
    "            skiprows=4,\n",
    "            usecols=[\n",
    "                \"Average Rt(min)\",\n",
    "                \"Average Mz\",\n",
    "                \"Metabolite name\",\n",
    "                \"Ontology\",\n",
    "                \"Annotation tag (VS1.0)\",\n",
    "                \"MS/MS matched\",\n",
    "                \"SP_10\",\n",
    "                \"SP_20\",\n",
    "            ],\n",
    "        )\n",
    "        .rename(columns=lambda x: re.sub(r\"\\W|[%/]\", \"_\", x.lower()))\n",
    "        .query(\"annotation_tag__vs1_0_ == '430' or annotation_tag__vs1_0_ == '530'\")\n",
    "        .query(query_str)\n",
    "        .drop_duplicates(subset=\"metabolite_name\")\n",
    "    )\n",
    "\n",
    "    # Create scatterplot\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"average_rt_min_\",\n",
    "        y=\"average_mz\",\n",
    "        alpha=0.5,\n",
    "        s=100,\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    # Set plot limits and turn off axis\n",
    "    plt.ylim(50, 750)\n",
    "    plt.xlim(0, 20)\n",
    "    plt.axis(False)\n",
    "\n",
    "    # Create the foreground image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"jpeg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    buf.seek(0)\n",
    "    foreground = Image.open(buf)\n",
    "    plt.close()\n",
    "\n",
    "    # Create the background image\n",
    "    background = (\n",
    "        Image.open(background_image_location)\n",
    "        .convert(\"RGB\")\n",
    "        .crop(crop_area)\n",
    "        .resize(foreground.size)\n",
    "    )\n",
    "\n",
    "    # Create a mask\n",
    "    mask = Image.new(\"L\", foreground.size, 128)\n",
    "\n",
    "    # Composite the images\n",
    "    composite_img = Image.composite(background, foreground, mask)\n",
    "\n",
    "    return composite_img, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a6fae-b484-4844-8ba2-ffc80698c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_img_pattern_1, pattern_df_1 = overlay_untargeted_data(\n",
    "    query_str=\"3.5<average_rt_min_<5 and 460<average_mz<500 or 5<average_rt_min_<6.5 and 575<average_mz<650 or 9.5<average_rt_min_<10 and 280<average_mz<320\",\n",
    "    ms_dial_location=\"PeakID_0_2023_12_25_17_39_09.csv\",\n",
    "    background_image_location=\"exp-5-plot_activation.png\",\n",
    "    crop_area=(25, 90, 850, 910),\n",
    ")\n",
    "\n",
    "composite_img_pattern_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b3163-0718-4439-a0ec-13847ff6a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_img_pattern_2, pattern_df_2 = overlay_untargeted_data(\n",
    "    query_str=\"(16.5<average_rt_min_<18 and 180<average_mz<250) or (4<average_rt_min_<5.5 and 50<average_mz<100) or (8<average_rt_min_<12 and 50<average_mz<100) or (8.5<average_rt_min_<9 and 250<average_mz<300) or (12<average_rt_min_<14 and 200<average_mz<220) or (18<average_rt_min_<20 and 280<average_mz<350)\",\n",
    "    ms_dial_location=\"PeakID_0_2023_12_25_17_39_09.csv\",\n",
    "    background_image_location=\"exp-5-plot_activation.png\",\n",
    "    crop_area=(20, 1070, 850, 1890),\n",
    ")\n",
    "\n",
    "composite_img_pattern_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dc66c-d16e-47e1-9f25-5d829a6a194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pattern_df_1, pattern_df_2], axis=0).query(\"annotation_tag__vs1_0_ == '430'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742400b-85e2-434a-b27e-d87039e42549",
   "metadata": {},
   "source": [
    "# Get in touch\n",
    "\n",
    "Did the app help with your research? Any ideas for making it better? Get in touch! I would love to hear from you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
