{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb309f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'DeepLCMS: Leveraging transfer learning for the classification of pseudo images in mass spectrometry-based analysis'\n",
    "author: Adam Cseresznye\n",
    "date: '2023-19-12'\n",
    "bibliography: DeepLCMS.bib\n",
    "categories:\n",
    "  - DeepLCMS\n",
    "jupyter: python3\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a692-e95d-4785-8d85-ed1cc10b4056",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Representative examples of DeepLCMS predictions accompanied by their corresponding probability estimates.](exp-5-prediction_matrix.png){fig-align=\"center\" width=50%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955da8b-ac66-47c9-95ba-2c66f68fe821",
   "metadata": {},
   "source": [
    "Welcome to DeepLCMS, a project that combines mass spectrometry analysis with the power of deep learning models!\n",
    "\n",
    "Unlike traditional methods, DeepLCMS eliminates the need for extensive data processing, including peak alignment, data annotation, quantitation, and other time-consuming steps. Instead, it relies on the power of deep learning to directly classify mass spectrometry-based pseudo-images with high accuracy. To demonstrate the capabilities of pre-trained neural networks for high-resolution LC/MS data, we successfully apply our convolutional neural network (CNN) to categorize substance abuse cases. We utilize the openly available Golestan Cohort Study's metabolomics HRMS/LC data to train and evaluate our CNN [@pourshams_cohort_2010; @ghanbari_metabolomics_2021; @li_untargeted_2020]. We also delve into the network's decision-making process through TorchCam library. This tool allows us to gain insights into the factors that influence the network's classifications, helping us identify key compound classes that play a crucial role in differentiating between classes. By analyzing retention time and molecular weight, we can pinpoint areas of interest within the data.\n",
    "\n",
    "DeepLCMS paves the way for a new era of mass spectrometry analysis, offering a faster, more efficient, and more insightful approach to data interpretation. Its ability to directly classify pseudo-images without extensive preprocessing opens up a world of possibilities for researchers and clinicians alike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4698f-d15a-4484-a3ff-692eb1e0685b",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "**At a glance**\n",
    "\n",
    "The DeepLCMS project aims to provide researchers with a reproducible source code for leveraging deep learning for mass spectrometry data analysis. It distinguishes itself from previous studies by:\n",
    "* Comparing Diverse Architecture Families: Assessing a broader range of architecture families to find the most suitable one, including cutting-edge architectures like vision transformers.\n",
    "* Hyperparameter Tuning: Conducting basic hyperparameter tuning to optimize the learning rate using Optuna including optimizer, and learning rate scheduler – crucial aspects beyond the architecture itself.\n",
    "* Image Quality Analysis: Investigating the impact of image quality on validation metrics, examining image sharpness and data augmentation imitating retention time shift.\n",
    "* Regularization Techniques: Employing regularization techniques like random-tilting images and random erasing during training to improve model generalization.\n",
    "* Interpreting Pretrained Network Decisions: Analyzing how the pre-trained network makes its decisions using TorchVision.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621f716-2c9d-4464-9f74-9a2d78a163c3",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d39a8-0d13-4104-aa4d-42ce70e259d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af189aa",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821aab42-6ea3-4277-9c47-01586fc1b53f",
   "metadata": {},
   "source": [
    "While computer vision has gained widespread adoption in various aspects of our lives[@dobson_birth_2023], its application in medical imaging and biosciences has lagged behind, primarily due to limitations in clinical dataset size, accessibility, privacy concerns, experimental complexity, and high acquisition costs. For such applications, transfer learning has emerged as a potential solution[@seddiki_towards_2020]. This technique is particularly effective with small datasets, requiring fewer computational resources while achieving good classification accuracy compared to models trained from scratch. Transfer learning involves a two-step process. Initially, a robust data representation is learned by training a model on a dataset comprising a vast amount of annotated data encompassing numerous categories (ImageNet for example). This representation is then utilized to construct a new model based on a smaller annotated dataset containing fewer categories. \n",
    "\n",
    "## Application of Pretrained Neural Networks for Mass Spectrometry Data\n",
    "\n",
    "The use of pre-trained neural networks for mass spectrometry data analysis is relatively new, with only a handful of publications available to date. These studies have demonstrated the potential of deep learning models to extract meaningful information from raw mass spectrometry data and perform predictive tasks without the need for extensive data processing as required by the traditional workflows.\n",
    "\n",
    "## Previous Research\n",
    "\n",
    "* In 2018, @behrmann_deep_2018 used deep learning techniques for tumor classification in Imaging Mass Spectrometry (IMS) data.\n",
    "\n",
    "* In 2020, @seddiki_towards_2020 utilized MALDI-TOF images of rat brain samples to assess the ability of three different CNN architectures – LeNet, Lecun, and VGG9 – to differentiate between different types of cancers based on their molecular profiles.\n",
    "\n",
    "* In 2021, @cadow_feasibility_2021 explored the use of pre-trained networks for the classification of tumors from normal prostate biopsies derived from SWATH-MS data. They delved into the potential of deep learning models for analyzing raw mass spectrometry data and performing predictive tasks without the need for protein quantification. To process raw MS images, the authors employed pre-trained neural network models to convert them into numerical vectors, enabling further processing. They then compared several classifiers, including logistic regression, support vector machines, and random forests, to accurately predict the phenotype.\n",
    "\n",
    "* In 2022, @shen_deep_2022 released deepPseudoMSI, a deep learning-based pseudo-mass spectrometry imaging platform, designed to predict the gestational age in pregnant women based on LC-MS-based metabolomics data. This application consists of two components: Pseudo-MS Image Converter: for converting LC-MS data into pseudo-images and the deep learning model itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e76a6e-4270-4af3-adbf-2294b5d81ce1",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Project Structure\n",
    "\n",
    "To accommodate the high computational demands of neural network training, the DeepLCMS project is divided into two main parts. The first part focuses on data preprocessing, specifically converting LC/MS data into pseudo-images using the PyOpenMS library, which is written in C++ and optimized for efficiency. This task can be handled on a CPU, and the corresponding source code is found in the `src/deeplcms_functions` directory.\n",
    "\n",
    "To effectively train the neural networks that demand GPU acceleration, the project employs the PyTorch Lightning framework, a comprehensive solution for building and deploying deep learning models. Training experiments are conducted within Jupyter Notebooks hosted on Google Colab, a cloud platform equipped with free GPU access. The training code and corresponding modules reside within the `src/train_google_colab` directory. This folder seamlessly integrates with Google Colab, allowing for effortless module imports.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09821cba-07d0-4a9a-81ee-ac2c1fc6d820",
   "metadata": {},
   "source": [
    "# Materials and Methods\n",
    "## Dataset\n",
    "\n",
    "To ensure the feasibility of our proof-of-concept demonstration, we selected a suitable dataset from the Metabolomics Workbench. We prioritized studies with distinct groups and a minimum sample size of 200. Additionally, we chose a dataset with a disk requirement of less than 50 GB to minimize computational resource demands. Based on these criteria, we identified the Golestan Cohort Study [@pourshams_cohort_2010]. This study, conducted in northeastern Iran, primarily investigates the risk factors for upper gastrointestinal cancers in this high-risk region. Approximately 50,000 volunteers were analyzed, including opium users and their mortality outcomes. Quantitative targeted liquid chromatography mass spectrometric (LC-MS/MS) data was collected at the University of North Carolina at Chapel Hill [@ghanbari_metabolomics_2021; @li_untargeted_2020]. The dataset consisted of 218 opioid users and 80 non-users. After initial data inspection and conversion to mzML format using the ProteoWizard 3.0.22155 software, files were divided into training (n = 214), validation (n = 54), and test (n = 30) sets. To evaluate the impact of image characteristics and augmentation techniques on classification performance, four datasets were prepared. One dataset employed a bin size of 500 × 500 using the numpy library's histogram2d function and was named `ST001618_Opium_study_LC_MS_500`. Another dataset incorporated data augmentation using the augment_images function, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. This dataset was named `ST001618_Opium_study_LC_MS_500_augmented`. The third dataset employed a higher bin size of 1000 × 1000 using histogram2d for sharper pseudoimages and was named `ST001618_Opium_study_LC_MS_1000`. The final dataset applied the same augmentation technique to this dataset, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. It was named `ST001618_Opium_study_LC_MS_1000_augmented`.\n",
    "\n",
    "## Software\n",
    "\n",
    "The source code for the DeepLCMS project utilizes the following software packages and versions: PyTorch Lightning 2.1.3, Pytorch Image Models (timm)  0.9.12, torchinfo 1.8.0, Optuna 3.5.0, TorchCam 0.4.0, pandas 2.1.3, NumPy 1.26, and Matplotlib 3.7.1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ade18-4cc3-4eca-aa38-48f089a609e1",
   "metadata": {},
   "source": [
    "# Results and Discussion\n",
    "## Selecting a model architecture family\n",
    "\n",
    "From the readily available model architecture families provided by [Pytorch Image Models](https://github.com/huggingface/pytorch-image-models#models), 68 unique ones were chosen as representative examples of a given class and filtered based on their parameter count, selecting those with parameters counts between 10 and 20 million to allow for an unbiased comparison. Out of the 68 ones selected 32 were subsequently underwent training with validation metrics recorded. According to @tbl-exp-1-result (arranged according to validation loss), the MobileOne S3 emerged as the top performer with an F1 score of 0.95. It was followed by DenseNet (F1 = 0.92), MobileViTV2 (F1 = 0.90), and ConvNeXt Nano (F1 = 0.84). RepVit M3 rounded out the top five with an F1 score of 0.86. To delve deeper into the performance of each architecture family, we evaluated all individual architectures within each family (@tbl-exp-1-best_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d41a1-373d-4436-b017-55b87fbab55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: tbl-exp-1-result\n",
    "# | tbl-cap: Validation metrics of model architectural families from the PyTorch Image Models library using models with parameter counts ranging from 10 to 20 million.\n",
    "\n",
    "(\n",
    "    pd.read_csv(\"exp-1-result.csv\")\n",
    "    .rename(\n",
    "        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n",
    "    )\n",
    "    .rename(columns={\"Minimal Param Model Count\": \"Parameter Count (M)\"})\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55c827-5f4b-4a52-9d8b-7df9f3bf3c55",
   "metadata": {},
   "source": [
    "As shown in @tbl-exp-1-best_models, the top four positions based on the validation metrics were occupied by models belonging to the ConvNeXt family. These models, particularly the first two (convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 with validation loss 0.19, convnext_large_mlp.clip_laion2b_augreg_ft_in1k with validation loss 0.22), were pre-trained on the extensive LAION-2B dataset and fine-tuned on ImageNet-1k, enabling them to learn complex patterns that generalize well to unseen data. This comprehensive dataset and model structure contribute to their superior performance in this task. The dominance of the ConvNeXt family is noteworthy, suggesting their effectiveness in handling complex data such as mass spectrometry pseudoimages. Apple's MobileOne also demonstrated remarkable results (validation loss = 0.27), ranking fourth in terms of validation loss. Finally, MobileViT (validation loss = 0.27), a lightweight network, secured the seventh position.  \n",
    "Nevertheless, it is essential to acknowledge that while the ConvNeXt model achieved the highest validation metrics, it is considerably larger and more parameter-intensive. This implies that it demands more computational resources for training and necessitates greater care to prevent potential overfitting, as opposed to other models that are 10-20 times smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e043125-7367-4fd7-817b-d0ef6ce2aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: tbl-exp-1-best_models\n",
    "# | tbl-cap: Top 10 models based on evaluation metrics of all model architectures in families from the PyTorch Image Models library, regardless of their parameter counts.\n",
    "\n",
    "(\n",
    "    pd.read_csv(\"exp-1-best_models.csv\")\n",
    "    .rename(\n",
    "        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n",
    "    )\n",
    "    .round(2)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cf409-b3ef-4e6e-926e-c7060066df19",
   "metadata": {},
   "source": [
    "To further assess the suitability of these models and the consistency of their performance, we trained the top three performing models from each family five consecutive times and calculated the median and standard deviation of their validation metrics (@fig-exp-2-replicates_result). This approach allowed us to identify the models that exhibited the most consistent performance across multiple training runs. The results revealed that the MobileViT family consistently performed similarly to the ConvNeXt across all five training runs, except for the validation losses where ConvNeXt exhibited the lowest values with 0.21. According to the replication study, the ConvNeXt emerged as the most effective model, surpassing both MobileOne and MobileViT2. Despite exhibiting similar patterns, ConvNeXt exhibited a statistically significant advantage over MobileViT in validation loss (p = 0.01) and achieved significantly better validation recall (p < 0.05). Among the three models tested, MobileOne consistently underperformed its counterparts in all performance metrics, except for validation recall, where it narrowly outperformed mobilevitv2 (p < 0.05). Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86307ea-d9d1-4919-a80f-e603073117de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: fig-exp-2-replicates_result\n",
    "# | fig-cap: \"Median and standard deviation of validation metrics achieved during consecutive trainings\"\n",
    "\n",
    "Image.open(\"exp-2-replicates_result.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c502a-e3d7-4d16-9683-acc3d849247c",
   "metadata": {},
   "source": [
    "# Get in touch\n",
    "\n",
    "Did the app help with your research? Any ideas for making it better? Get in touch! I would love to hear from you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a7090-8354-495f-b068-23ae990fcb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
